{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "etD6ywks4aDV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(dict): \n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file):\n",
        "        with open(file, 'r') as f:\n",
        "            config = json.loads(f.read())\n",
        "            return Config(config)\n",
        "    \n",
        "    @classmethod\n",
        "    def save(cls, config_dict, file):\n",
        "        with open(file, 'w') as f:\n",
        "            config = json.dump(config_dict, f, indent=4)\n",
        "            print('config file is saved on {}'.format(file))"
      ],
      "metadata": {
        "id": "A27Zd_BbdCf7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_dict = {\n",
        "    'vocab_size' : 30000, # tokenizer.vocab_size,\n",
        "    'd_model' : 768,\n",
        "    'max_position_embeddings' : 512,\n",
        "    'type_vocab_size' : 2,\n",
        "    'num_labels' : 2,\n",
        "    'pad_token_id' : 0, # tokenizer.pad_token_id,\n",
        "    'bos_token_id' : 2, # tokenizer.bos_token_id,\n",
        "    'eos_token_id' : 3, # tokenizer.eos_token_id,\n",
        "    'share_embedding' : True,\n",
        "    'init_std' : 2e-2,\n",
        "    'layer_norm_eps' : 1e-12, \n",
        "    'drop_out_raito' : 0.1,\n",
        "    'num_enc_layers' : 12,\n",
        "    'num_dec_layers' : 12,\n",
        "    'num_att_heads' : 12,\n",
        "    'feed_forward_dim' : 2048,\n",
        "    'has_relative_attention_bias' : True, # Only T5\n",
        "    'relative_attention_num_buckets' : 32, # Only T5\n",
        "}\n",
        "\n",
        "config = Config(config_dict)"
      ],
      "metadata": {
        "id": "Y1rcPVKRAhGb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([2,4,5,6,7,8,9,3,0,0]).unsqueeze(0)\n",
        "token_type_ids = torch.tensor([0]*10).unsqueeze(0)\n",
        "attention_mask = torch.tensor([1]*8 + [0]*2).unsqueeze(0)\n",
        "dec_input_ids = torch.tensor([2,4,5,6,3,0,0]).unsqueeze(0)"
      ],
      "metadata": {
        "id": "vCKNfQdvT58P"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids.shape, token_type_ids.shape, attention_mask.shape, dec_input_ids.shape"
      ],
      "metadata": {
        "id": "zv3z9faDkiY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ec8ff22-d602-4687-bd0d-3e7c373d62cf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 10]),\n",
              " torch.Size([1, 10]),\n",
              " torch.Size([1, 10]),\n",
              " torch.Size([1, 7]))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Taeksu-Kim/Various_Transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA-Mo2y7hXJB",
        "outputId": "9c7d2e90-5460-4112-c792-0de190346aec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Various_Transformers' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Various_Transformers/original_transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNNkMFj4kibe",
        "outputId": "372a6727-2fca-46f2-83ba-41e8c88558d0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Various_Transformers/original_transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformer import *"
      ],
      "metadata": {
        "id": "LyV-2G4MkieF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Pid24hNL4-k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(config)"
      ],
      "metadata": {
        "id": "ku5gdoHhkigf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model(input_ids,\n",
        "               dec_input_ids)"
      ],
      "metadata": {
        "id": "c9SzmJLskijM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM6fnKE_kzF-",
        "outputId": "ab56489a-7377-45b1-ec3b-92869d8d6730"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 30000])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def get_extended_attention_mask(attention_mask, autoregressive=False):\n",
        "\n",
        "    dtype = torch.float16\n",
        "\n",
        "    extended_attention_mask = attention_mask[:, None, None, :]\n",
        "\n",
        "    if autoregressive is True:\n",
        "\n",
        "      subsequent_mask = torch.ones_like(extended_attention_mask, device=attention_mask.device).expand(-1, -1, attention_mask.size(1), -1)\n",
        "      subsequent_mask = subsequent_mask.triu(diagonal=1)\n",
        "      subsequent_mask = torch.lt(subsequent_mask,1)\n",
        "\n",
        "      extended_attention_mask = torch.gt((extended_attention_mask+subsequent_mask), 1).int()\n",
        "\n",
        "    extended_attention_mask = extended_attention_mask.to(dtype=dtype)  # fp16 compatibility\n",
        "    extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n",
        "    return extended_attention_mask\n",
        "\n",
        "class PoswiseFeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(PoswiseFeedForward, self).__init__()      \n",
        "\n",
        "        self.feed_forward = nn.Sequential(nn.Linear(config.d_model, config.feed_forward_dim),\n",
        "                                          nn.GELU(),\n",
        "                                          nn.Linear(config.feed_forward_dim, config.d_model),\n",
        "                                          nn.Dropout(config.drop_out_raito))\n",
        "    def forward(self, inputs):\n",
        "        return self.feed_forward(inputs)\n",
        "    \n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.d_model, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.d_model)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.d_model)\n",
        "\n",
        "        self.LayerNorm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.drop_out_raito)\n",
        "\n",
        "        self.position_ids = torch.arange(config.max_position_embeddings).expand((1, -1))\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        input_ids=None, \n",
        "        token_type_ids=None, \n",
        "        position_ids=None,\n",
        "        ):\n",
        "        \n",
        "        batch_size, seq_len = input_ids.size()\n",
        "        device = input_ids.device\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros([batch_size, seq_len], dtype=torch.long, device=device)\n",
        "\n",
        "        position_ids = self.position_ids[:, :seq_len].to(device)\n",
        "\n",
        "        word_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        embeddings = word_embeds + token_type_embeddings + position_embeddings\n",
        "        \n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "class BertModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "      super().__init__()\n",
        "      self.embedding = BertEmbeddings(config)\n",
        "      self.encoder = BertEncoder(config, self.embedding)\n",
        "\n",
        "      self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize weights for each layer\n",
        "        self.apply(self.init_layer_weights)\n",
        "\n",
        "    # ref huggingface\n",
        "    # https://huggingface.co/transformers/v4.9.2/_modules/transformers/models/electra/modeling_electra.html#ElectraPreTrainedModel\n",
        "    def init_layer_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.init_std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.init_std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "            module.eps = self.config.layer_norm_eps\n",
        "    \n",
        "    def forward(self,\n",
        "                input_ids,\n",
        "                token_type_ids=None,\n",
        "                attention_mask=None,\n",
        "                ):\n",
        "\n",
        "      outputs, self_attn_probs, _ = self.encoder(input_ids=input_ids,\n",
        "                                                 token_type_ids=token_type_ids,\n",
        "                                                 attention_mask=attention_mask,\n",
        "                                                 )\n",
        "      \n",
        "      return outputs, self_attn_probs\n",
        "\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config, embedding):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embedding = embedding\n",
        "        self.layers = nn.ModuleList(\n",
        "            [BertEncoderLayer(config) for _ in range(config.num_enc_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids,\n",
        "                token_type_ids,\n",
        "                attention_mask,\n",
        "                ):\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.ne(self.config.pad_token_id).int()\n",
        "        \n",
        "        self_attention_mask = get_extended_attention_mask(attention_mask, autoregressive=False)\n",
        "\n",
        "        outputs = self.embedding(input_ids,\n",
        "                                 token_type_ids=token_type_ids)\n",
        "        \n",
        "        self_attn_probs = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            outputs, self_attn_prob = layer(inputs=outputs,\n",
        "                                            self_attention_mask=self_attention_mask,\n",
        "                                            )\n",
        "            self_attn_probs.append(self_attn_prob)\n",
        "\n",
        "        return outputs, self_attn_probs, self_attention_mask    \n",
        "    \n",
        "class BertEncoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self_attention = BertAttention(config)\n",
        "        self.attention_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)\n",
        "        self.feed_forward = PoswiseFeedForward(config)\n",
        "        self.feed_forward_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        inputs, \n",
        "        self_attention_mask,\n",
        "        ):\n",
        "\n",
        "        outputs, self_attn_prob = self.self_attention(query=inputs, \n",
        "                                                      key=None, \n",
        "                                                      value=None, \n",
        "                                                      attention_mask=self_attention_mask,\n",
        "                                                      )\n",
        "        outputs = self.attention_norm(inputs + outputs)\n",
        "\n",
        "        inputs = outputs\n",
        "        outputs = self.feed_forward(inputs)\n",
        "        outputs = self.feed_forward_norm(inputs + outputs)\n",
        "        \n",
        "        return outputs, self_attn_prob\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_model = config.d_model\n",
        "        self.num_att_heads = config.num_att_heads\n",
        "        assert self.d_model % self.num_att_heads == 0, \"d_model({}) % num_att_heads({}) = {}. It should be 0.\".format(self.d_model, self.num_att_heads, self.d_model % self.num_att_heads)\n",
        "        self.d_head = int(self.d_model / self.num_att_heads)\n",
        "        self.scale = self.d_head ** 0.5\n",
        "        \n",
        "        self.query_proj = nn.Linear(self.d_model, self.num_att_heads * self.d_head)\n",
        "        self.key_proj = nn.Linear(self.d_model, self.num_att_heads * self.d_head)\n",
        "        self.value_proj = nn.Linear(self.d_model, self.num_att_heads * self.d_head)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(config.drop_out_raito)\n",
        "\n",
        "        self.fc = nn.Linear(self.d_head * self.num_att_heads, self.d_model)\n",
        "        self.context_dropout = nn.Dropout(config.drop_out_raito)\n",
        "\n",
        "    def forward(self,\n",
        "                query,\n",
        "                key=None,\n",
        "                value=None,\n",
        "                attention_mask=None,\n",
        "                ):\n",
        "\n",
        "        if key is None and value is None:\n",
        "            key = value = query\n",
        "\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        query = self.query_proj(query).view(batch_size, -1, self.num_att_heads, self.d_head).transpose(1,2) # [bs, num_heads, query_len, d_head]\n",
        "        key = self.key_proj(key).view(batch_size, -1, self.num_att_heads, self.d_head).transpose(1,2) # [bs, num_heads, key_len, d_head]\n",
        "        value = self.value_proj(value).view(batch_size, -1, self.num_att_heads, self.d_head).transpose(1,2) # [bs, num_heads, value_len, d_head]\n",
        "\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / self.scale # [bs, num_heads, query_len, key_len]        \n",
        "        scores = scores + attention_mask\n",
        "        \n",
        "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
        "        attn_prob = self.attn_dropout(attn_prob)\n",
        "\n",
        "        context = torch.matmul(attn_prob, value) # [bs, num_heads, query_len, d_head]\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_att_heads * self.d_head)\n",
        "        \n",
        "        context = self.fc(context)\n",
        "        context = self.context_dropout(context)\n",
        "\n",
        "        return context, attn_prob\n",
        "\n",
        "class BertDecoder(nn.Module):\n",
        "    def __init__(self, config, embedding):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embedding = embedding\n",
        "        self.layers = nn.ModuleList(\n",
        "            [BertDecoderLayer(config) for _ in range(config.num_enc_layers)]\n",
        "        )\n",
        "        self.fc = nn.Linear(config.d_model, config.vocab_size)\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids,\n",
        "                attention_mask=None,\n",
        "                enc_outputs=None,\n",
        "                enc_attention_mask=None):\n",
        "      \n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.ne(self.config.pad_token_id).int()\n",
        "\n",
        "        self_attention_mask = get_extended_attention_mask(attention_mask, autoregressive=True)\n",
        "\n",
        "        outputs = self.embedding(input_ids,\n",
        "                                 token_type_ids=None)\n",
        "\n",
        "        self_attn_probs, cross_attn_probs = [], []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            outputs, self_attn_prob, cross_attn_prob = layer(inputs=outputs,\n",
        "                                                             self_attention_mask=self_attention_mask,\n",
        "                                                             enc_outputs=enc_outputs,\n",
        "                                                             cross_attention_mask=enc_attention_mask,\n",
        "                                                             )\n",
        "            self_attn_probs.append(self_attn_prob)\n",
        "            cross_attn_probs.append(cross_attn_prob)      \n",
        "\n",
        "        outputs = self.fc(outputs)        \n",
        "\n",
        "        return outputs, self_attn_probs, cross_attn_probs\n",
        "\n",
        "class BertDecoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self_attention = BertAttention(config)\n",
        "        self.self_attention_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)\n",
        "\n",
        "        self.cross_attention = BertAttention(config)\n",
        "        self.cross_attention_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)\n",
        "\n",
        "        self.feed_forward = PoswiseFeedForward(config)\n",
        "        self.feed_forward_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)\n",
        "        \n",
        "    def forward(self,\n",
        "                inputs,\n",
        "                self_attention_mask,\n",
        "                enc_outputs,\n",
        "                cross_attention_mask,\n",
        "                ):\n",
        "\n",
        "        outputs, self_attn_prob = self.self_attention(query=inputs, \n",
        "                                                      key=None, \n",
        "                                                      value=None, \n",
        "                                                      attention_mask=self_attention_mask,\n",
        "                                                      )\n",
        "        outputs = self.self_attention_norm(inputs + outputs)\n",
        "\n",
        "        inputs = outputs\n",
        "        outputs, cross_attn_prob = self.cross_attention(query=inputs, \n",
        "                                                        key=enc_outputs, \n",
        "                                                        value=enc_outputs, \n",
        "                                                        attention_mask=cross_attention_mask,\n",
        "                                                        )\n",
        "        outputs = self.cross_attention_norm(inputs + outputs)\n",
        "\n",
        "        inputs = outputs\n",
        "        outputs = self.feed_forward(inputs)\n",
        "        outputs = self.feed_forward_norm(inputs + outputs)\n",
        "        \n",
        "        return outputs, self_attn_prob, cross_attn_prob\n",
        "\n",
        "class Bert_Encoder_Decoder_Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "      super().__init__()\n",
        "      self.config=config\n",
        "\n",
        "      if config.share_embedding is True:\n",
        "          self.shared_embedding = BertEmbeddings(config)\n",
        "          self.encoder = BertEncoder(config, self.shared_embedding)\n",
        "          self.decoder = BertDecoder(config, self.shared_embedding)\n",
        "      \n",
        "      else:\n",
        "          self.encoder_embedding = BertEmbeddings(config)\n",
        "          self.decoder_embedding = BertEmbeddings(config)\n",
        "  \n",
        "          self.encoder = BertEncoder(config, self.encoder_embedding)\n",
        "          self.decoder = BertDecoder(config, self.decoder_embedding)\n",
        "\n",
        "      self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize weights for each layer\n",
        "        self.apply(self.init_layer_weights)\n",
        "\n",
        "    # ref huggingface\n",
        "    # https://huggingface.co/transformers/v4.9.2/_modules/transformers/models/electra/modeling_electra.html#ElectraPreTrainedModel\n",
        "    def init_layer_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.init_std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.init_std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "            module.eps = self.config.layer_norm_eps\n",
        "\n",
        "    def forward(self,\n",
        "                enc_input_ids,\n",
        "                enc_token_type_ids=None,\n",
        "                enc_attention_mask=None,\n",
        "                dec_input_ids=None,\n",
        "                dec_attention_mask=None,\n",
        "                ):\n",
        "\n",
        "        enc_outputs, enc_self_attn_probs, enc_attention_mask = self.encoder(enc_input_ids,\n",
        "                                                                            enc_token_type_ids,\n",
        "                                                                            enc_attention_mask,\n",
        "                                                                            )\n",
        "        \n",
        "        dec_outputs, dec_self_attn_probs, dec_cross_attn_probs = self.decoder(input_ids=dec_input_ids,\n",
        "                                                                              attention_mask=dec_attention_mask,\n",
        "                                                                              enc_outputs=enc_outputs,\n",
        "                                                                              enc_attention_mask=enc_attention_mask,\n",
        "                                                                              )\n",
        "\n",
        "        return dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_cross_attn_probs\n"
      ],
      "metadata": {
        "id": "V-kHNN8MkzIh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../bert_style"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdyC_QlBkzK-",
        "outputId": "a07882a3-3d0c-4953-9d60-28324e9ed6ca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Various_Transformers/bert_style\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bert import *"
      ],
      "metadata": {
        "id": "LcmbK3kzkil3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def get_extended_attention_mask(attention_mask, autoregressive=False):\n",
        "\n",
        "    dtype = torch.float16\n",
        "\n",
        "    extended_attention_mask = attention_mask[:, None, None, :]\n",
        "\n",
        "    if autoregressive is True:\n",
        "\n",
        "      subsequent_mask = torch.ones_like(extended_attention_mask, device=attention_mask.device).expand(-1, -1, attention_mask.size(1), -1)\n",
        "      subsequent_mask = subsequent_mask.triu(diagonal=1)\n",
        "      subsequent_mask = torch.lt(subsequent_mask,1)\n",
        "\n",
        "      extended_attention_mask = torch.gt((extended_attention_mask+subsequent_mask), 1).int()\n",
        "\n",
        "    extended_attention_mask = extended_attention_mask.to(dtype=dtype)  # fp16 compatibility\n",
        "    extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n",
        "    return extended_attention_mask\n",
        "\n",
        "class PoswiseFeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(PoswiseFeedForward, self).__init__()      \n",
        "\n",
        "        self.feed_forward = nn.Sequential(nn.Linear(config.d_model, config.feed_forward_dim),\n",
        "                                          nn.GELU(),\n",
        "                                          nn.Linear(config.feed_forward_dim, config.d_model),\n",
        "                                          nn.Dropout(config.drop_out_raito))\n",
        "    def forward(self, inputs):\n",
        "        return self.feed_forward(inputs)\n",
        "    \n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.d_model, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.d_model)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.d_model)\n",
        "\n",
        "        self.LayerNorm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.drop_out_raito)\n",
        "\n",
        "        self.position_ids = torch.arange(config.max_position_embeddings).expand((1, -1))\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        input_ids=None, \n",
        "        token_type_ids=None, \n",
        "        position_ids=None,\n",
        "        ):\n",
        "        \n",
        "        batch_size, seq_len = input_ids.size()\n",
        "        device = input_ids.device\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros([batch_size, seq_len], dtype=torch.long, device=device)\n",
        "\n",
        "        position_ids = self.position_ids[:, :seq_len].to(device)\n",
        "\n",
        "        word_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        embeddings = word_embeds + token_type_embeddings + position_embeddings\n",
        "        \n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "class BertModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.embedding = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config, self.embedding)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize weights for each layer\n",
        "        self.apply(self.init_layer_weights)\n",
        "\n",
        "    # ref huggingface\n",
        "    # https://huggingface.co/transformers/v4.9.2/_modules/transformers/models/electra/modeling_electra.html#ElectraPreTrainedModel\n",
        "    def init_layer_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.init_std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.init_std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "            module.eps = self.config.layer_norm_eps\n",
        "    \n",
        "    def forward(self,\n",
        "                input_ids,\n",
        "                token_type_ids=None,\n",
        "                attention_mask=None,\n",
        "                ):\n",
        "\n",
        "      outputs, self_attn_probs, _ = self.encoder(input_ids=input_ids,\n",
        "                                                 token_type_ids=token_type_ids,\n",
        "                                                 attention_mask=attention_mask,\n",
        "                                                 )\n",
        "      \n",
        "      return outputs, self_attn_probs\n",
        "\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config, embedding):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embedding = embedding\n",
        "        self.layers = nn.ModuleList(\n",
        "            [BertEncoderLayer(config) for _ in range(config.num_enc_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids,\n",
        "                token_type_ids,\n",
        "                attention_mask,\n",
        "                ):\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.ne(self.config.pad_token_id).int()\n",
        "        \n",
        "        self_attention_mask = get_extended_attention_mask(attention_mask, autoregressive=False)\n",
        "\n",
        "        outputs = self.embedding(input_ids,\n",
        "                                 token_type_ids=token_type_ids)\n",
        "        \n",
        "        self_attn_probs = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            outputs, self_attn_prob = layer(inputs=outputs,\n",
        "                                            self_attention_mask=self_attention_mask,\n",
        "                                            )\n",
        "            self_attn_probs.append(self_attn_prob)\n",
        "\n",
        "        return outputs, self_attn_probs, self_attention_mask    \n",
        "    \n",
        "class BertEncoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self_attention = BertAttention(config)\n",
        "        self.attention_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)\n",
        "        self.feed_forward = PoswiseFeedForward(config)\n",
        "        self.feed_forward_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        inputs, \n",
        "        self_attention_mask,\n",
        "        ):\n",
        "\n",
        "        outputs, self_attn_prob = self.self_attention(query=inputs, \n",
        "                                                      key=None, \n",
        "                                                      value=None, \n",
        "                                                      attention_mask=self_attention_mask,\n",
        "                                                      )\n",
        "        outputs = self.attention_norm(inputs + outputs)\n",
        "\n",
        "        inputs = outputs\n",
        "        outputs = self.feed_forward(inputs)\n",
        "        outputs = self.feed_forward_norm(inputs + outputs)\n",
        "        \n",
        "        return outputs, self_attn_prob\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_model = config.d_model\n",
        "        self.num_att_heads = config.num_att_heads\n",
        "        assert self.d_model % self.num_att_heads == 0, \"d_model({}) % num_att_heads({}) = {}. It should be 0.\".format(self.d_model, self.num_att_heads, self.d_model % self.num_att_heads)\n",
        "        self.d_head = int(self.d_model / self.num_att_heads)\n",
        "        self.scale = self.d_head ** 0.5\n",
        "        \n",
        "        self.query_proj = nn.Linear(self.d_model, self.num_att_heads * self.d_head)\n",
        "        self.key_proj = nn.Linear(self.d_model, self.num_att_heads * self.d_head)\n",
        "        self.value_proj = nn.Linear(self.d_model, self.num_att_heads * self.d_head)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(config.drop_out_raito)\n",
        "\n",
        "        self.fc = nn.Linear(self.d_head * self.num_att_heads, self.d_model)\n",
        "        self.context_dropout = nn.Dropout(config.drop_out_raito)\n",
        "\n",
        "    def forward(self,\n",
        "                query,\n",
        "                key=None,\n",
        "                value=None,\n",
        "                attention_mask=None,\n",
        "                ):\n",
        "\n",
        "        if key is None and value is None:\n",
        "            key = value = query\n",
        "\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        query = self.query_proj(query).view(batch_size, -1, self.num_att_heads, self.d_head).transpose(1,2) # [bs, num_heads, query_len, d_head]\n",
        "        key = self.key_proj(key).view(batch_size, -1, self.num_att_heads, self.d_head).transpose(1,2) # [bs, num_heads, key_len, d_head]\n",
        "        value = self.value_proj(value).view(batch_size, -1, self.num_att_heads, self.d_head).transpose(1,2) # [bs, num_heads, value_len, d_head]\n",
        "\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / self.scale # [bs, num_heads, query_len, key_len]        \n",
        "        scores = scores + attention_mask\n",
        "        \n",
        "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
        "        attn_prob = self.attn_dropout(attn_prob)\n",
        "\n",
        "        context = torch.matmul(attn_prob, value) # [bs, num_heads, query_len, d_head]\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_att_heads * self.d_head)\n",
        "        \n",
        "        context = self.fc(context)\n",
        "        context = self.context_dropout(context)\n",
        "\n",
        "        return context, attn_prob\n",
        "\n",
        "class BertDecoder(nn.Module):\n",
        "    def __init__(self, config, embedding):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embedding = embedding\n",
        "        self.layers = nn.ModuleList(\n",
        "            [BertDecoderLayer(config) for _ in range(config.num_enc_layers)]\n",
        "        )\n",
        "        self.fc = nn.Linear(config.d_model, config.vocab_size)\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids,\n",
        "                attention_mask=None,\n",
        "                enc_outputs=None,\n",
        "                enc_attention_mask=None):\n",
        "      \n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.ne(self.config.pad_token_id).int()\n",
        "\n",
        "        self_attention_mask = get_extended_attention_mask(attention_mask, autoregressive=True)\n",
        "\n",
        "        outputs = self.embedding(input_ids,\n",
        "                                 token_type_ids=None)\n",
        "\n",
        "        self_attn_probs, cross_attn_probs = [], []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            outputs, self_attn_prob, cross_attn_prob = layer(inputs=outputs,\n",
        "                                                             self_attention_mask=self_attention_mask,\n",
        "                                                             enc_outputs=enc_outputs,\n",
        "                                                             cross_attention_mask=enc_attention_mask,\n",
        "                                                             )\n",
        "            self_attn_probs.append(self_attn_prob)\n",
        "            cross_attn_probs.append(cross_attn_prob)      \n",
        "\n",
        "        outputs = self.fc(outputs)        \n",
        "\n",
        "        return outputs, self_attn_probs, cross_attn_probs\n",
        "\n",
        "class BertDecoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self_attention = BertAttention(config)\n",
        "        self.self_attention_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)\n",
        "\n",
        "        self.cross_attention = BertAttention(config)\n",
        "        self.cross_attention_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)\n",
        "\n",
        "        self.feed_forward = PoswiseFeedForward(config)\n",
        "        self.feed_forward_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)\n",
        "        \n",
        "    def forward(self,\n",
        "                inputs,\n",
        "                self_attention_mask,\n",
        "                enc_outputs,\n",
        "                cross_attention_mask,\n",
        "                ):\n",
        "\n",
        "        outputs, self_attn_prob = self.self_attention(query=inputs, \n",
        "                                                      key=None, \n",
        "                                                      value=None, \n",
        "                                                      attention_mask=self_attention_mask,\n",
        "                                                      )\n",
        "        outputs = self.self_attention_norm(inputs + outputs)\n",
        "\n",
        "        inputs = outputs\n",
        "        outputs, cross_attn_prob = self.cross_attention(query=inputs, \n",
        "                                                        key=enc_outputs, \n",
        "                                                        value=enc_outputs, \n",
        "                                                        attention_mask=cross_attention_mask,\n",
        "                                                        )\n",
        "        outputs = self.cross_attention_norm(inputs + outputs)\n",
        "\n",
        "        inputs = outputs\n",
        "        outputs = self.feed_forward(inputs)\n",
        "        outputs = self.feed_forward_norm(inputs + outputs)\n",
        "        \n",
        "        return outputs, self_attn_prob, cross_attn_prob\n",
        "\n",
        "class Bert_Encoder_Decoder_Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "      super().__init__()\n",
        "      self.config=config\n",
        "\n",
        "      if config.share_embedding is True:\n",
        "          self.shared_embedding = BertEmbeddings(config)\n",
        "          self.encoder = BertEncoder(config, self.shared_embedding)\n",
        "          self.decoder = BertDecoder(config, self.shared_embedding)\n",
        "      \n",
        "      else:\n",
        "          self.encoder_embedding = BertEmbeddings(config)\n",
        "          self.decoder_embedding = BertEmbeddings(config)\n",
        "  \n",
        "          self.encoder = BertEncoder(config, self.encoder_embedding)\n",
        "          self.decoder = BertDecoder(config, self.decoder_embedding)\n",
        "\n",
        "      self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize weights for each layer\n",
        "        self.apply(self.init_layer_weights)\n",
        "\n",
        "    # ref huggingface\n",
        "    # https://huggingface.co/transformers/v4.9.2/_modules/transformers/models/electra/modeling_electra.html#ElectraPreTrainedModel\n",
        "    def init_layer_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.init_std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.init_std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "            module.eps = self.config.layer_norm_eps\n",
        "\n",
        "    def forward(self,\n",
        "                enc_input_ids,\n",
        "                enc_token_type_ids=None,\n",
        "                enc_attention_mask=None,\n",
        "                dec_input_ids=None,\n",
        "                dec_attention_mask=None,\n",
        "                ):\n",
        "\n",
        "        enc_outputs, enc_self_attn_probs, enc_attention_mask = self.encoder(enc_input_ids,\n",
        "                                                                            enc_token_type_ids,\n",
        "                                                                            enc_attention_mask,\n",
        "                                                                            )\n",
        "        \n",
        "        dec_outputs, dec_self_attn_probs, dec_cross_attn_probs = self.decoder(input_ids=dec_input_ids,\n",
        "                                                                              attention_mask=dec_attention_mask,\n",
        "                                                                              enc_outputs=enc_outputs,\n",
        "                                                                              enc_attention_mask=enc_attention_mask,\n",
        "                                                                              )\n",
        "\n",
        "        return dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_cross_attn_probs"
      ],
      "metadata": {
        "id": "GMqYpHr6b6Ov"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel(config)"
      ],
      "metadata": {
        "id": "aIgZT1zrkiof"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model(input_ids,\n",
        "               token_type_ids,\n",
        "               attention_mask,\n",
        "               )"
      ],
      "metadata": {
        "id": "PxVlC09Rl6bP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEgqQv6Wl6d3",
        "outputId": "22194b49-2172-490d-faa6-39522892b453"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Bert_Encoder_Decoder_Model(config)"
      ],
      "metadata": {
        "id": "4lwhB09-l6gs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model(input_ids,\n",
        "               token_type_ids,\n",
        "               attention_mask,\n",
        "               dec_input_ids,\n",
        "               )"
      ],
      "metadata": {
        "id": "EVi0S8KXl6ja"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8-CmwxQl6mL",
        "outputId": "cf22e866-1c98-4297-df46-03c85f4f6dbd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 30000])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ZFLS3PMl6oq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../gpt1_style"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laJoG398l6rM",
        "outputId": "d74384ec-5e91-467a-a89c-1c5fe3383e0d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Various_Transformers/gpt1_style\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt1 import *"
      ],
      "metadata": {
        "id": "alSSA_D3l6t4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT1Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embedding = GPT1Embeddings(config)\n",
        "        self.decoder = GPT1Decoder(config, self.embedding)\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids,\n",
        "                token_type_ids=None,\n",
        "                attention_mask=None,\n",
        "                ):\n",
        "      \n",
        "        outputs, self_attn_probs = self.decoder(input_ids,\n",
        "                                                token_type_ids,\n",
        "                                                attention_mask,\n",
        "                                                )\n",
        "\n",
        "        return outputs, self_attn_probs"
      ],
      "metadata": {
        "id": "rRnKQ9_oRHhl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT1Decoder(nn.Module):\n",
        "    def __init__(self, config, embedding):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embedding = embedding\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [GPT1DecoderLayer(config) for i in range(config.num_dec_layers)]\n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids,\n",
        "                token_type_ids=None,\n",
        "                attention_mask=None,\n",
        "                ):\n",
        "      \n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.ne(self.config.pad_token_id).int()\n",
        "\n",
        "        self_attention_mask = get_extended_attention_mask(attention_mask, autoregressive=True)\n",
        "\n",
        "        outputs = self.embedding(input_ids,\n",
        "                                 token_type_ids=token_type_ids)\n",
        "\n",
        "        self_attn_probs = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            outputs, self_attn_prob = layer(inputs=outputs,\n",
        "                                            self_attention_mask=self_attention_mask, \n",
        "                                            )\n",
        "            self_attn_probs.append(self_attn_prob)       \n",
        "\n",
        "        outputs = self.fc(outputs)\n",
        "\n",
        "        return outputs, self_attn_probs"
      ],
      "metadata": {
        "id": "hIJLVUs4O-28"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT1Embeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.d_model, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.d_model)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.drop_out_raito)\n",
        "\n",
        "        self.position_ids = torch.arange(config.max_position_embeddings).expand((1, -1))\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        input_ids=None, \n",
        "        token_type_ids=None, \n",
        "        position_ids=None,\n",
        "        ):\n",
        "        \n",
        "        batch_size, seq_len = input_ids.size()\n",
        "        device = input_ids.device\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros([batch_size, seq_len], dtype=torch.long, device=device)\n",
        "\n",
        "        position_ids = self.position_ids[:, :seq_len].to(device)\n",
        "\n",
        "        inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        embeddings = inputs_embeds + token_type_embeddings + position_embeddings\n",
        "        \n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "FQ5h2LgIOAZV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT1Model(config)"
      ],
      "metadata": {
        "id": "Od8NEd9nmWho"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model(dec_input_ids)"
      ],
      "metadata": {
        "id": "T5CPOfGlmWj2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQbSdprzmWme",
        "outputId": "05f48103-b9e8-453b-edb8-46bb4e5e36c6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 30000])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../gpt2_style"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGpLDsbfmWo8",
        "outputId": "dca1df9c-552f-4dba-cbe1-d0be73a8bebd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Various_Transformers/gpt2_style\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt2 import *"
      ],
      "metadata": {
        "id": "g2PZv-NwmjUc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2Model(config)"
      ],
      "metadata": {
        "id": "dVBjEKFhmjWt"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model(dec_input_ids)"
      ],
      "metadata": {
        "id": "f4MqRdrVmjZV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Djj0c6Mlmjb9",
        "outputId": "ee3ad7cc-8db8-4a74-bfe8-f95605880754"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 30000])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../t5_style"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HRWyYE0mWrW",
        "outputId": "cabb6845-0c72-4dea-f9a9-9d792efccd12"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Various_Transformers/t5_style\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from t5 import *"
      ],
      "metadata": {
        "id": "9rjGCyJFhXOZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5_Model(config)"
      ],
      "metadata": {
        "id": "TpTN2PzHb1wy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model(input_ids,\n",
        "               attention_mask,\n",
        "               dec_input_ids,\n",
        "               )"
      ],
      "metadata": {
        "id": "eXBqGSJ9cApb"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IzU-2NfcAso",
        "outputId": "8af97d34-7838-4e10-82aa-4afda21c9870"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 30000])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ebXc5FQtyI1g"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}