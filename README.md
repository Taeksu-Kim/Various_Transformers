# Various_Transformers

- 여러가지 Transformer들의 응용에 대해 정리한 repo
- Huggingface의 코드를 모델의 기본 코드로 가정 후, Model 별로 변수 이름 및 기본적인 구조를 통일하는 방식으로 구현
- PLM은 Pre-train 전략까지 같이 다뤄야 의미가 있지만 Pre-train 전략은 Model에 종속적이지 않고, 다양한 전략이 있으므로 여기서는 Model의 구조만을 다룸

## Usage
Various_Transformers.ipynb

## 작업 완료
- Original Transformer   
- Bert Style
- GPT1 Style
- GPT2 Style
- T5 Style

